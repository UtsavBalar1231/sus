{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SUS - Simple Universal Scraper","text":"<p>Async documentation scraper for converting websites to Markdown format. Built with Python 3.12+, httpx, and asyncio.</p>"},{"location":"#what-is-sus","title":"What is SUS?","text":"<p>SUS (Simple Universal Scraper) is a config-driven web scraper for converting documentation websites to Markdown format with preserved assets. Built with Python 3.12+ using httpx and asyncio, it controls crawling through YAML configuration files with regex/glob/prefix pattern matching, token bucket rate limiting, and dual-level concurrency controls.</p> <p>Key features:</p> <ul> <li>httpx async HTTP client with asyncio for concurrent page fetching</li> <li>Pydantic 2.9+ validated YAML configuration files</li> <li>Token bucket rate limiting (configurable req/s with burst capacity)</li> <li>Dual concurrency: global (10) + per-domain (2) connection limits</li> <li>markdownify-based HTML \u2192 Markdown with YAML frontmatter</li> <li>Link rewriting to relative paths calculated by directory depth</li> <li>Concurrent asset downloads (images, CSS, JS) with SHA-256 deduplication</li> <li>Rich terminal UI with real-time crawl statistics and progress tracking</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone &lt;repo-url&gt;\ncd sus\n\n# Install dependencies with uv\nuv sync\n\n# Verify installation\nuv run sus --version\n</code></pre>"},{"location":"#your-first-scrape","title":"Your First Scrape","text":"<pre><code># Scrape with example config (limit to 10 pages for testing)\nuv run sus scrape --config examples/aptly.yaml --max-pages 10\n\n# Full scrape (no page limit)\nuv run sus scrape --config examples/aptly.yaml\n</code></pre>"},{"location":"#create-your-own-config","title":"Create Your Own Config","text":"<pre><code># Interactive configuration wizard\nuv run sus init my-config.yaml\n\n# Validate your config\nuv run sus validate my-config.yaml\n\n# Run the scraper\nuv run sus scrape --config my-config.yaml\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>This documentation is organized into three main sections:</p>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Configuration Guide - Learn how to configure scrapers with YAML files</li> <li>CLI Reference - Command-line interface documentation</li> <li>Crawler Guide - Understanding the crawling engine</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>Complete API documentation auto-generated from source code docstrings. See the API Overview for a full module listing.</p>"},{"location":"#development","title":"Development","text":"<p>For contributors and developers:</p> <ul> <li>Architecture - System design and implementation phases</li> <li>Contributing - How to contribute to the project</li> <li>Testing - Running tests and type checking</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Offline documentation mirrors with relative links and preserved assets</li> <li>Documentation archival for compliance and auditing</li> <li>Legacy HTML documentation conversion to Markdown format</li> <li>Custom documentation processing pipelines with configurable output structure</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.12 or higher</li> <li>uv (recommended) or pip</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is currently unlicensed. Please contact the maintainer for licensing information.</p>"},{"location":"#core-dependencies","title":"Core Dependencies","text":"<ul> <li>httpx 0.28+ - HTTP/2 async client for page fetching</li> <li>Pydantic 2.9+ - YAML config validation with type coercion</li> <li>Typer 0.15+ - CLI argument parsing and command routing</li> <li>Rich 14+ - Terminal progress bars and formatted output</li> <li>markdownify 0.14+ - HTML to Markdown parser</li> <li>lxml 5.3+ - Fast HTML parsing for link extraction</li> </ul>"},{"location":"api/assets/","title":"Assets","text":""},{"location":"api/assets/#sus.assets","title":"assets","text":"<p>Asset downloading and management.</p> <p>Handles concurrent downloading of web assets (images, CSS, JavaScript) with progress tracking and SHA-256 content deduplication. Provides AssetDownloader for async downloads with configurable concurrency limits.</p>"},{"location":"api/assets/#sus.assets-classes","title":"Classes","text":""},{"location":"api/assets/#sus.assets.Asset","title":"Asset  <code>dataclass</code>","text":"<pre><code>Asset(url: str, type: Literal['image', 'css', 'js', 'font'], original_src: str)\n</code></pre> <p>Represents an asset to download.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>str</code> <p>Asset URL (absolute)</p> <code>type</code> <code>Literal['image', 'css', 'js', 'font']</code> <p>Asset type (image, css, js, font)</p> <code>original_src</code> <code>str</code> <p>Original src attribute from HTML</p>"},{"location":"api/assets/#sus.assets.AssetStats","title":"AssetStats  <code>dataclass</code>","text":"<pre><code>AssetStats(downloaded: int = 0, failed: int = 0, skipped: int = 0, total_bytes: int = 0, errors: dict[str, int] = dict())\n</code></pre> <p>Statistics for asset downloads.</p> <p>Tracks download success/failure counts and total bytes.</p>"},{"location":"api/assets/#sus.assets.AssetDownloader","title":"AssetDownloader","text":"<pre><code>AssetDownloader(config: AssetConfig, output_manager: OutputManager, client: AsyncClient | None = None)\n</code></pre> <p>Downloads assets concurrently with error handling.</p> <p>Features: - Concurrent downloads with semaphore limiting - Skip existing files (idempotent) - Comprehensive error tracking - Progress tracking via stats</p> <p>Initialize asset downloader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AssetConfig</code> <p>AssetConfig from SusConfig</p> required <code>output_manager</code> <code>OutputManager</code> <p>OutputManager instance (for path resolution)</p> required <code>client</code> <code>AsyncClient | None</code> <p>Optional HTTP client (for testing with mocks)</p> <code>None</code>"},{"location":"api/assets/#sus.assets.AssetDownloader-functions","title":"Functions","text":""},{"location":"api/assets/#sus.assets.AssetDownloader.download_all","title":"download_all  <code>async</code>","text":"<pre><code>download_all(assets: list[str]) -&gt; AssetStats\n</code></pre> <p>Download all assets concurrently.</p> <p>Parameters:</p> Name Type Description Default <code>assets</code> <code>list[str]</code> <p>List of asset URLs to download</p> required <p>Returns:</p> Type Description <code>AssetStats</code> <p>AssetStats with download results</p> <p>Logic: 1. Filter out already downloaded assets 2. Create HTTP client if not provided 3. Create async tasks for each asset 4. Gather all tasks (use asyncio.gather with return_exceptions=True) 5. Update stats based on results 6. Close client if we created it</p>"},{"location":"api/cli/","title":"CLI","text":""},{"location":"api/cli/#sus.cli","title":"cli","text":"<p>Command line interface.</p> <p>Provides the CLI for SUS using Typer with Rich-formatted output. Main commands: scrape, validate, init, and list. All commands use Rich for styled terminal output.</p>"},{"location":"api/cli/#sus.cli-classes","title":"Classes","text":""},{"location":"api/cli/#sus.cli-functions","title":"Functions","text":""},{"location":"api/cli/#sus.cli.version_callback","title":"version_callback","text":"<pre><code>version_callback(value: bool) -&gt; None\n</code></pre> <p>Print version and exit.</p>"},{"location":"api/cli/#sus.cli.main","title":"main","text":"<pre><code>main(version: bool | None = typer.Option(None, '--version', '-V', help='Show version and exit', callback=version_callback, is_eager=True)) -&gt; None\n</code></pre> <p>SUS - Simple Universal Scraper for documentation sites.</p>"},{"location":"api/cli/#sus.cli.scrape","title":"scrape","text":"<pre><code>scrape(config: Path = typer.Option(..., '--config', '-c', help='Path to YAML config file', exists=True, file_okay=True, dir_okay=False, resolve_path=True), output: str | None = typer.Option(None, '--output', '-o', help='Override output directory'), verbose: bool = typer.Option(False, '--verbose', '-v', help='Verbose logging'), dry_run: bool = typer.Option(False, '--dry-run', help=\"Don't write any files (simulation mode)\"), max_pages: int | None = typer.Option(None, '--max-pages', help='Limit number of pages to crawl', min=1), preview: bool = typer.Option(False, '--preview', help='Export dry-run JSON report')) -&gt; None\n</code></pre> <p>Scrape documentation site using config file.</p> <p>Loads configuration from YAML file and runs the scraper according to the specified rules. Use --dry-run to preview without writing files.</p>"},{"location":"api/cli/#sus.cli.validate","title":"validate","text":"<pre><code>validate(config_path: Path = typer.Argument(..., help='Path to YAML config file to validate', exists=True, file_okay=True, dir_okay=False, resolve_path=True)) -&gt; None\n</code></pre> <p>Validate a SUS configuration file.</p> <p>Checks YAML syntax and validates all configuration fields against the schema. Displays detailed error messages if validation fails.</p>"},{"location":"api/cli/#sus.cli.init","title":"init","text":"<pre><code>init(output_path: Path | None = typer.Argument(None, help='Output path for generated config file (default: config.yaml)'), force: bool = typer.Option(False, '--force', '-f', help='Overwrite existing file')) -&gt; None\n</code></pre> <p>Create a new SUS configuration file interactively.</p> <p>Prompts for basic configuration options and generates a minimal YAML config file. Use this as a starting point for your scraper.</p>"},{"location":"api/cli/#sus.cli.list_examples","title":"list_examples","text":"<pre><code>list_examples() -&gt; None\n</code></pre> <p>List example configurations from the examples/ directory.</p> <p>Shows all available example configs with their descriptions and start URLs. Use these as templates for your own configurations.</p>"},{"location":"api/config/","title":"Configuration","text":""},{"location":"api/config/#sus.config","title":"config","text":"<p>Configuration system.</p> <p>YAML configuration files validated by Pydantic models. Provides type-safe configuration with validation, sensible defaults, and clear error messages. Main function: load_config().</p>"},{"location":"api/config/#sus.config-classes","title":"Classes","text":""},{"location":"api/config/#sus.config.PathPattern","title":"PathPattern","text":"<p>               Bases: <code>BaseModel</code></p> <p>URL pattern matching configuration.</p> <p>Supports three pattern types: - regex: Regular expression matching - glob: Shell-style glob patterns (e.g., \"*.html\") - prefix: Simple prefix matching (e.g., \"/docs/\")</p>"},{"location":"api/config/#sus.config.PathPattern-functions","title":"Functions","text":""},{"location":"api/config/#sus.config.PathPattern.matches","title":"matches","text":"<pre><code>matches(path: str) -&gt; bool\n</code></pre> <p>Check if URL path matches this pattern.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>URL path component (e.g., \"/docs/guide/\")</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path matches the pattern</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pattern = PathPattern(pattern=\"/docs/\", type=\"prefix\")\n&gt;&gt;&gt; pattern.matches(\"/docs/guide/\")\nTrue\n&gt;&gt;&gt; pattern.matches(\"/blog/\")\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = PathPattern(pattern=\"*.html\", type=\"glob\")\n&gt;&gt;&gt; pattern.matches(\"/page.html\")\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; pattern = PathPattern(pattern=r\"^/api/v\\d+/\", type=\"regex\")\n&gt;&gt;&gt; pattern.matches(\"/api/v2/users\")\nTrue\n</code></pre>"},{"location":"api/config/#sus.config.SiteConfig","title":"SiteConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Website configuration for crawling.</p>"},{"location":"api/config/#sus.config.CrawlingRules","title":"CrawlingRules","text":"<p>               Bases: <code>BaseModel</code></p> <p>Crawling behavior configuration.</p> <p>Controls which URLs to follow, concurrency limits, rate limiting, and retry behavior.</p>"},{"location":"api/config/#sus.config.PathMappingConfig","title":"PathMappingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>URL to file path mapping configuration.</p>"},{"location":"api/config/#sus.config.MarkdownConfig","title":"MarkdownConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Markdown conversion options.</p>"},{"location":"api/config/#sus.config.OutputConfig","title":"OutputConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output directory structure configuration.</p>"},{"location":"api/config/#sus.config.AssetConfig","title":"AssetConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Asset download configuration.</p>"},{"location":"api/config/#sus.config.SusConfig","title":"SusConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Main configuration model for SUS scraper.</p> <p>This is the root configuration object that contains all settings for a scraping project.</p>"},{"location":"api/config/#sus.config.SusConfig-functions","title":"Functions","text":""},{"location":"api/config/#sus.config.SusConfig.validate_name","title":"validate_name  <code>classmethod</code>","text":"<pre><code>validate_name(v: str) -&gt; str\n</code></pre> <p>Validate that name is a valid directory name.</p> <p>The name must not contain path separators or special characters that would be invalid in directory names.</p>"},{"location":"api/config/#sus.config-functions","title":"Functions","text":""},{"location":"api/config/#sus.config.load_config","title":"load_config","text":"<pre><code>load_config(path: Path) -&gt; SusConfig\n</code></pre> <p>Load and validate YAML configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to YAML configuration file</p> required <p>Returns:</p> Type Description <code>SusConfig</code> <p>Validated SusConfig instance</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If config file is not found, invalid YAML, or validation fails</p>"},{"location":"api/converter/","title":"Converter","text":""},{"location":"api/converter/#sus.converter","title":"converter","text":"<p>HTML to Markdown conversion.</p> <p>Converts HTML documentation to Markdown with YAML frontmatter. Provides SusMarkdownConverter (custom markdownify with alt text preservation) and ContentConverter (high-level orchestrator).</p>"},{"location":"api/converter/#sus.converter-classes","title":"Classes","text":""},{"location":"api/converter/#sus.converter.SusMarkdownConverter","title":"SusMarkdownConverter","text":"<p>               Bases: <code>MarkdownConverter</code></p> <p>Custom Markdown converter with better handling for docs.</p> <p>Overrides specific conversion methods for improved output quality.</p>"},{"location":"api/converter/#sus.converter.SusMarkdownConverter-functions","title":"Functions","text":""},{"location":"api/converter/#sus.converter.SusMarkdownConverter.convert_img","title":"convert_img","text":"<pre><code>convert_img(el: Any, text: str, **kwargs: Any) -&gt; str\n</code></pre> <p>Override image conversion for better alt text handling.</p> <p>Preserves alt text when present; uses empty string when absent to avoid None formatting issues in markdown output.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>Any</code> <p>HTML image element</p> required <code>text</code> <code>str</code> <p>Converted text content (unused for images)</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments from parent class (e.g., convert_as_inline)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Markdown image syntax: </p> <p>Examples:</p> <p> \u2192   \u2192 </p>"},{"location":"api/converter/#sus.converter.SusMarkdownConverter.convert_pre","title":"convert_pre","text":"<pre><code>convert_pre(el: Any, text: str, **kwargs: Any) -&gt; str\n</code></pre> <p>Override code block conversion with language detection.</p> <p>Detects language from class attribute (e.g., class=\"language-python\") and formats as fenced code blocks with language specifier.</p> <p>Parameters:</p> Name Type Description Default <code>el</code> <code>Any</code> <p>HTML pre element</p> required <code>text</code> <code>str</code> <p>Converted text content</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments from parent class (e.g., parent_tags)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Markdown fenced code block with language</p> <p>Examples:</p> <pre><code>print(\"hello\")</code></pre> <p>\u2192 <code>python \u2192 print(\"hello\") \u2192</code></p> <pre><code>plain text</code></pre> <p>\u2192 <code>\u2192 plain text \u2192</code></p>"},{"location":"api/converter/#sus.converter.ContentConverter","title":"ContentConverter","text":"<pre><code>ContentConverter(config: MarkdownConfig)\n</code></pre> <p>Converts HTML to Markdown with frontmatter.</p> <p>Handles HTML cleaning, markdown conversion, frontmatter generation, and markdown post-processing.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>MarkdownConfig containing conversion options</p> <code>converter</code> <p>SusMarkdownConverter instance for HTML\u2192Markdown conversion</p> <p>Initialize converter with markdown config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MarkdownConfig</code> <p>MarkdownConfig from SusConfig</p> required"},{"location":"api/converter/#sus.converter.ContentConverter-functions","title":"Functions","text":""},{"location":"api/converter/#sus.converter.ContentConverter.convert","title":"convert","text":"<pre><code>convert(html: str, url: str, title: str | None = None, metadata: dict[str, Any] | None = None) -&gt; str\n</code></pre> <p>Convert HTML to Markdown with frontmatter.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content to convert</p> required <code>url</code> <code>str</code> <p>Source URL (for frontmatter)</p> required <code>title</code> <code>str | None</code> <p>Page title (extracted from  or provided)&lt;/p&gt;               &lt;/div&gt;             &lt;/td&gt;             &lt;td&gt;                   &lt;code&gt;None&lt;/code&gt;             &lt;/td&gt;           &lt;/tr&gt;           &lt;tr class=\"doc-section-item\"&gt;             &lt;td&gt;                 &lt;code&gt;metadata&lt;/code&gt;             &lt;/td&gt;             &lt;td&gt;                   &lt;code&gt;&lt;span title=\"dict\"&gt;dict&lt;/span&gt;[&lt;span title=\"str\"&gt;str&lt;/span&gt;, &lt;span title=\"typing.Any\"&gt;Any&lt;/span&gt;] | None&lt;/code&gt;             &lt;/td&gt;             &lt;td&gt;               &lt;div class=\"doc-md-description\"&gt;                 &lt;p&gt;Additional metadata for frontmatter&lt;/p&gt;               &lt;/div&gt;             &lt;/td&gt;             &lt;td&gt;                   &lt;code&gt;None&lt;/code&gt;             &lt;/td&gt;           &lt;/tr&gt;       &lt;/tbody&gt;     &lt;/table&gt;       &lt;p&gt;&lt;span class=\"doc-section-title\"&gt;Returns:&lt;/span&gt;&lt;/p&gt;     &lt;table&gt;       &lt;thead&gt;         &lt;tr&gt;           &lt;th&gt;Type&lt;/th&gt;           &lt;th&gt;Description&lt;/th&gt;         &lt;/tr&gt;       &lt;/thead&gt;       &lt;tbody&gt;           &lt;tr class=\"doc-section-item\"&gt;             &lt;td&gt;                   &lt;code&gt;&lt;span title=\"str\"&gt;str&lt;/span&gt;&lt;/code&gt;             &lt;/td&gt;             &lt;td&gt;               &lt;div class=\"doc-md-description\"&gt;                 &lt;p&gt;Markdown content with YAML frontmatter&lt;/p&gt;               &lt;/div&gt;             &lt;/td&gt;           &lt;/tr&gt;       &lt;/tbody&gt;     &lt;/table&gt;   &lt;p&gt;&lt;span class=\"doc-section-title\"&gt;Examples:&lt;/span&gt;&lt;/p&gt;     &lt;div class=\"language-pycon highlight\"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span id=\"__span-0-1\"&gt;&lt;a id=\"__codelineno-0-1\" name=\"__codelineno-0-1\" href=\"#__codelineno-0-1\"&gt;&lt;/a&gt;&lt;span class=\"gp\"&gt;&gt;&gt;&gt; &lt;/span&gt;&lt;span class=\"n\"&gt;converter&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;ContentConverter&lt;/span&gt;&lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;MarkdownConfig&lt;/span&gt;&lt;span class=\"p\"&gt;())&lt;/span&gt; &lt;/span&gt;&lt;span id=\"__span-0-2\"&gt;&lt;a id=\"__codelineno-0-2\" name=\"__codelineno-0-2\" href=\"#__codelineno-0-2\"&gt;&lt;/a&gt;&lt;span class=\"gp\"&gt;&gt;&gt;&gt; &lt;/span&gt;&lt;span class=\"n\"&gt;html&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"s1\"&gt;'&lt;html&gt;&lt;head&gt;&lt;title&gt;Test&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;'&lt;/span&gt; &lt;/span&gt;&lt;span id=\"__span-0-3\"&gt;&lt;a id=\"__codelineno-0-3\" name=\"__codelineno-0-3\" href=\"#__codelineno-0-3\"&gt;&lt;/a&gt;&lt;span class=\"gp\"&gt;&gt;&gt;&gt; &lt;/span&gt;&lt;span class=\"n\"&gt;result&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;converter&lt;/span&gt;&lt;span class=\"o\"&gt;.&lt;/span&gt;&lt;span class=\"n\"&gt;convert&lt;/span&gt;&lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;html&lt;/span&gt;&lt;span class=\"p\"&gt;,&lt;/span&gt; &lt;span class=\"s2\"&gt;\"https://example.com/test\"&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt; &lt;/span&gt;&lt;span id=\"__span-0-4\"&gt;&lt;a id=\"__codelineno-0-4\" name=\"__codelineno-0-4\" href=\"#__codelineno-0-4\"&gt;&lt;/a&gt;&lt;span class=\"gp\"&gt;&gt;&gt;&gt; &lt;/span&gt;&lt;span class=\"s2\"&gt;\"# Hello\"&lt;/span&gt; &lt;span class=\"ow\"&gt;in&lt;/span&gt; &lt;span class=\"n\"&gt;result&lt;/span&gt; &lt;/span&gt;&lt;span id=\"__span-0-5\"&gt;&lt;a id=\"__codelineno-0-5\" name=\"__codelineno-0-5\" href=\"#__codelineno-0-5\"&gt;&lt;/a&gt;&lt;span class=\"go\"&gt;True&lt;/span&gt; &lt;/span&gt;&lt;span id=\"__span-0-6\"&gt;&lt;a id=\"__codelineno-0-6\" name=\"__codelineno-0-6\" href=\"#__codelineno-0-6\"&gt;&lt;/a&gt;&lt;span class=\"gp\"&gt;&gt;&gt;&gt; &lt;/span&gt;&lt;span class=\"s2\"&gt;\"title: Test\"&lt;/span&gt; &lt;span class=\"ow\"&gt;in&lt;/span&gt; &lt;span class=\"n\"&gt;result&lt;/span&gt; &lt;/span&gt;&lt;span id=\"__span-0-7\"&gt;&lt;a id=\"__codelineno-0-7\" name=\"__codelineno-0-7\" href=\"#__codelineno-0-7\"&gt;&lt;/a&gt;&lt;span class=\"go\"&gt;True&lt;/span&gt; &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;         &lt;p&gt;Steps: 1. Extract title from HTML if not provided (from &lt;title&gt; tag) 2. Convert HTML to Markdown using SusMarkdownConverter 3. Clean markdown (remove excessive blank lines, fix spacing) 4. Add frontmatter if configured 5. Return final markdown&lt;/p&gt;       &lt;/div&gt;  &lt;/div&gt;      &lt;/div&gt;      &lt;/div&gt;  &lt;/div&gt;       &lt;/div&gt;      &lt;/div&gt;  &lt;/div&gt;"},{"location":"api/crawler/","title":"Crawler","text":""},{"location":"api/crawler/#sus.crawler","title":"crawler","text":"<p>Async web crawler.</p> <p>Async HTTP crawling with token bucket rate limiting, concurrency control, and robots.txt compliance. Provides Crawler (queue-based async crawler) and RateLimiter (burst-friendly throttling).</p>"},{"location":"api/crawler/#sus.crawler-classes","title":"Classes","text":""},{"location":"api/crawler/#sus.crawler.RateLimiter","title":"RateLimiter","text":"<pre><code>RateLimiter(rate: float, burst: int = 5)\n</code></pre> <p>Token bucket rate limiter for burst-friendly rate limiting.</p> <p>The token bucket algorithm allows for bursts of requests while maintaining an average rate limit over time. Tokens are added to the bucket at a constant rate, and each request consumes one token.</p> Example <p>limiter = RateLimiter(rate=2.0, burst=5) await limiter.acquire()  # Consumes 1 token</p> <p>Initialize rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>rate</code> <code>float</code> <p>Requests per second (e.g., 2.0 = 0.5s average delay)</p> required <code>burst</code> <code>int</code> <p>Maximum burst size (tokens in bucket)</p> <code>5</code>"},{"location":"api/crawler/#sus.crawler.RateLimiter-functions","title":"Functions","text":""},{"location":"api/crawler/#sus.crawler.RateLimiter.acquire","title":"acquire  <code>async</code>","text":"<pre><code>acquire() -&gt; None\n</code></pre> <p>Acquire a token, waiting if necessary.</p> <p>Implements the token bucket algorithm: 1. Calculate tokens added since last update (time_passed * rate) 2. Add tokens (cap at burst size) 3. If tokens &gt;= 1, consume token and return 4. Otherwise, sleep until next token available</p>"},{"location":"api/crawler/#sus.crawler.CrawlResult","title":"CrawlResult  <code>dataclass</code>","text":"<pre><code>CrawlResult(url: str, html: str, status_code: int, content_type: str, links: list[str], assets: list[str])\n</code></pre> <p>Result from crawling a single page.</p> <p>Contains the page content, metadata, and extracted links/assets.</p>"},{"location":"api/crawler/#sus.crawler.CrawlerStats","title":"CrawlerStats  <code>dataclass</code>","text":"<pre><code>CrawlerStats(pages_crawled: int = 0, pages_failed: int = 0, assets_discovered: int = 0, total_bytes: int = 0, start_time: float = (lambda: asyncio.get_event_loop().time())(), error_counts: dict[str, int] = dict())\n</code></pre> <p>Statistics collected during crawl.</p> <p>Tracks pages crawled, failures, bytes downloaded, and errors by type.</p>"},{"location":"api/crawler/#sus.crawler.RobotsTxtChecker","title":"RobotsTxtChecker","text":"<pre><code>RobotsTxtChecker(client: AsyncClient, user_agent: str = 'SUS/0.1.0')\n</code></pre> <p>Checks robots.txt files to determine if URLs can be crawled.</p> <p>Caches robots.txt files per domain to avoid re-fetching. On fetch errors, defaults to allowing the URL (graceful degradation).</p> Example <p>checker = RobotsTxtChecker(client, user_agent=\"MyBot/1.0\") allowed = await checker.is_allowed(\"https://example.com/page\")</p> <p>Initialize robots.txt checker.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncClient</code> <p>HTTP client for fetching robots.txt files</p> required <code>user_agent</code> <code>str</code> <p>User agent string to use for checking rules</p> <code>'SUS/0.1.0'</code>"},{"location":"api/crawler/#sus.crawler.RobotsTxtChecker-functions","title":"Functions","text":""},{"location":"api/crawler/#sus.crawler.RobotsTxtChecker.is_allowed","title":"is_allowed  <code>async</code>","text":"<pre><code>is_allowed(url: str) -&gt; bool\n</code></pre> <p>Check if URL is allowed by robots.txt.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if allowed (or on fetch error), False if disallowed</p>"},{"location":"api/crawler/#sus.crawler.Crawler","title":"Crawler","text":"<pre><code>Crawler(config: SusConfig, client: AsyncClient | None = None)\n</code></pre> <p>Async web crawler with rate limiting and concurrency control.</p> <p>Features: - Token bucket rate limiting for burst-friendly rate control - Global and per-domain concurrency limits - Exponential backoff retry logic - Dependency injection for testability - Content-type aware handling</p> Example <p>config = load_config(Path(\"config.yaml\")) crawler = Crawler(config) async for result in crawler.crawl(): ...     print(f\"Crawled: {result.url}\")</p> <p>Initialize crawler.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SusConfig</code> <p>Validated configuration</p> required <code>client</code> <code>AsyncClient | None</code> <p>Optional HTTP client (for testing with mocks)</p> <code>None</code>"},{"location":"api/crawler/#sus.crawler.Crawler-functions","title":"Functions","text":""},{"location":"api/crawler/#sus.crawler.Crawler.crawl","title":"crawl  <code>async</code>","text":"<pre><code>crawl() -&gt; AsyncGenerator[CrawlResult, None]\n</code></pre> <p>Crawl pages starting from start_urls.</p> <p>Implements queue-based crawling with concurrency control. Pages are fetched in parallel up to the configured concurrency limits, and new links are added to the queue as they are discovered.</p> <p>Yields:</p> Type Description <code>AsyncGenerator[CrawlResult, None]</code> <p>CrawlResult for each successfully crawled page</p>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#sus.exceptions","title":"exceptions","text":"<p>Custom exceptions for SUS.</p>"},{"location":"api/exceptions/#sus.exceptions-classes","title":"Classes","text":""},{"location":"api/exceptions/#sus.exceptions.SusError","title":"SusError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all SUS errors.</p>"},{"location":"api/exceptions/#sus.exceptions.ConfigError","title":"ConfigError","text":"<p>               Bases: <code>SusError</code></p> <p>Raised when configuration is invalid or cannot be loaded.</p>"},{"location":"api/exceptions/#sus.exceptions.CrawlError","title":"CrawlError","text":"<p>               Bases: <code>SusError</code></p> <p>Raised when crawling fails.</p>"},{"location":"api/outputs/","title":"Output","text":""},{"location":"api/outputs/#sus.outputs","title":"outputs","text":"<p>Output path mapping and link rewriting.</p> <p>Transforms web URLs to local file paths and rewrites absolute links to relative paths for offline browsing. Provides OutputManager for all path operations and markdown link rewriting.</p>"},{"location":"api/outputs/#sus.outputs-classes","title":"Classes","text":""},{"location":"api/outputs/#sus.outputs.OutputManager","title":"OutputManager","text":"<pre><code>OutputManager(config: SusConfig, dry_run: bool = False)\n</code></pre> <p>Manages output paths and link rewriting.</p> <p>Handles: - URL to file path mapping - Directory structure creation - Relative link calculation - Link rewriting in markdown</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = SusConfig(...)\n&gt;&gt;&gt; manager = OutputManager(config)\n&gt;&gt;&gt; doc_path = manager.get_doc_path(\"https://example.com/doc/guide/\")\n&gt;&gt;&gt; asset_path = manager.get_asset_path(\"https://example.com/img/logo.png\")\n&gt;&gt;&gt; markdown = manager.rewrite_links(markdown, source_url)\n</code></pre> <p>Initialize output manager.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SusConfig</code> <p>Validated SusConfig instance</p> required <code>dry_run</code> <code>bool</code> <p>If True, don't create directories or write files</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If config is invalid or paths are malformed</p>"},{"location":"api/outputs/#sus.outputs.OutputManager-functions","title":"Functions","text":""},{"location":"api/outputs/#sus.outputs.OutputManager.get_doc_path","title":"get_doc_path","text":"<pre><code>get_doc_path(url: str) -&gt; Path\n</code></pre> <p>Convert URL to markdown file path.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Source URL to convert</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Absolute path to markdown file</p> <p>Logic: 1. Parse URL and extract path component 2. Strip configured prefix (config.output.path_mapping.strip_prefix) 3. Handle directory URLs (ending in /) \u2192 use index_file 4. Convert path segments to file path 5. Ensure .md extension 6. Create parent directories (unless dry run)</p> <p>Examples:</p> <p>URL: https://example.com/doc/guide/install/ strip_prefix: /doc \u2192 docs/guide/install/index.md</p> <p>URL: https://example.com/doc/overview strip_prefix: /doc \u2192 docs/overview.md</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If URL cannot be parsed or path is invalid</p>"},{"location":"api/outputs/#sus.outputs.OutputManager.get_asset_path","title":"get_asset_path","text":"<pre><code>get_asset_path(asset_url: str) -&gt; Path\n</code></pre> <p>Convert asset URL to file path.</p> <p>Parameters:</p> Name Type Description Default <code>asset_url</code> <code>str</code> <p>Asset URL (image, CSS, JS, etc.)</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Absolute path to asset file</p> <p>Logic: 1. Parse URL and extract path 2. Preserve original directory structure under assets_dir 3. Example: https://example.com/img/logo.png \u2192 assets/img/logo.png</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; manager.get_asset_path(\"https://example.com/img/logo.png\")\nPath(\"/path/to/output/site/assets/img/logo.png\")\n</code></pre> <pre><code>&gt;&gt;&gt; manager.get_asset_path(\"https://example.com/css/style.css\")\nPath(\"/path/to/output/site/assets/css/style.css\")\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If URL cannot be parsed or path is invalid</p>"},{"location":"api/outputs/#sus.outputs.OutputManager.rewrite_links","title":"rewrite_links","text":"<pre><code>rewrite_links(markdown: str, source_url: str) -&gt; str\n</code></pre> <p>Rewrite links in markdown to relative paths.</p> <p>Parameters:</p> Name Type Description Default <code>markdown</code> <code>str</code> <p>Markdown content with absolute URLs</p> required <code>source_url</code> <code>str</code> <p>URL of the source page (for calculating relative paths)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Markdown with rewritten links</p> <p>Rewrites two types of links: 1. Internal doc links: text \u2192 text 2. Asset links:  \u2192 </p> <p>Logic: 1. Calculate source file path from URL 2. Find all markdown links: text and  3. For each link:    - If internal doc link (same domain, under allowed paths):      * Calculate target doc path      * Calculate relative path from source to target    - If asset link (image, css, js):      * Calculate asset path      * Calculate relative path from source to asset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; markdown = \"[Guide](https://example.com/doc/guide/)\"\n&gt;&gt;&gt; result = manager.rewrite_links(markdown, \"https://example.com/doc/\")\n&gt;&gt;&gt; print(result)\n\"[Guide](guide/index.md)\"\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source_url is invalid</p>"},{"location":"api/overview/","title":"API Reference","text":"<p>Complete API documentation auto-generated from source code docstrings.</p> <p>All modules are fully typed with mypy --strict compliance and Google-style docstring documentation.</p>"},{"location":"api/overview/#core-modules","title":"Core Modules","text":""},{"location":"api/overview/#configuration-layer","title":"Configuration Layer","text":"<ul> <li>config - Configuration system with Pydantic models and YAML loading</li> <li>exceptions - Custom exception hierarchy</li> </ul>"},{"location":"api/overview/#crawling-url-handling-layer","title":"Crawling &amp; URL Handling Layer","text":"<ul> <li>crawler - Async web crawler with rate limiting and concurrency control</li> <li>rules - URL filtering, normalization, and link extraction</li> </ul>"},{"location":"api/overview/#content-processing-layer","title":"Content Processing Layer","text":"<ul> <li>converter - HTML to Markdown conversion with frontmatter</li> <li>outputs - File path mapping and link rewriting</li> <li>assets - Concurrent asset downloading</li> </ul>"},{"location":"api/overview/#orchestration-layer","title":"Orchestration Layer","text":"<ul> <li>scraper - Main pipeline orchestrator</li> <li>cli - Typer-based command-line interface</li> </ul>"},{"location":"api/overview/#utilities","title":"Utilities","text":"<ul> <li>utils - Shared utility functions</li> </ul>"},{"location":"api/overview/#usage-patterns","title":"Usage Patterns","text":"<p>All modules follow consistent patterns:</p> <ol> <li>Type Safety: Full type hints with mypy --strict compliance</li> <li>Async Support: Async/await patterns using httpx and asyncio</li> <li>Documentation: Google-style docstrings with examples and type annotations</li> <li>Pydantic Validation: Configuration validated with Pydantic error messages</li> </ol>"},{"location":"api/overview/#getting-started-with-the-api","title":"Getting Started with the API","text":"<p>For most use cases, you'll interact with:</p> <ol> <li>config.load_config() - Load YAML configuration</li> <li>Crawler - Async web crawler</li> <li>run_scraper() - Main orchestration function</li> </ol>"},{"location":"api/overview/#architecture-overview","title":"Architecture Overview","text":"<p>SUS implements a six-phase pipeline architecture:</p> <ol> <li>Configuration System (<code>config.py</code>) - Pydantic 2.9+ models with YAML validation</li> <li>Crawler Engine (<code>crawler.py</code>) - httpx async client with token bucket rate limiter</li> <li>URL Filtering (<code>rules.py</code>) - lxml-based link extraction with pattern matching</li> <li>Content Conversion (<code>converter.py</code>) - markdownify HTML parser with frontmatter</li> <li>CLI Interface (<code>cli.py</code>) - Typer commands with Rich progress bars</li> <li>Testing (<code>tests/</code>) - 139+ pytest tests with mypy --strict compliance</li> </ol> <p>See individual module documentation for detailed API references.</p>"},{"location":"api/rules/","title":"URL Rules","text":""},{"location":"api/rules/#sus.rules","title":"rules","text":"<p>URL filtering and crawling rules.</p> <p>URL normalization, validation, and rule-based filtering for controlling crawl scope. Provides URLNormalizer (URL consistency), RulesEngine (pattern matching), and LinkExtractor (HTML parsing).</p>"},{"location":"api/rules/#sus.rules-classes","title":"Classes","text":""},{"location":"api/rules/#sus.rules.URLNormalizer","title":"URLNormalizer","text":"<p>Centralized URL normalization and validation.</p> <p>Utilities for: - Normalizing URLs (lowercase scheme/hostname, remove default ports, strip fragments) - Filtering dangerous schemes (javascript:, data:, file:, etc.) - Handling query parameters (strip or preserve strategies)</p>"},{"location":"api/rules/#sus.rules.URLNormalizer-functions","title":"Functions","text":""},{"location":"api/rules/#sus.rules.URLNormalizer.normalize_url","title":"normalize_url  <code>staticmethod</code>","text":"<pre><code>normalize_url(url: str) -&gt; str\n</code></pre> <p>Normalize URL for consistent handling.</p> <p>Normalizations applied: - Convert scheme to lowercase (HTTP \u2192 http) - Convert hostname to lowercase (Example.COM \u2192 example.com) - Remove default ports (http://example.com:80 \u2192 http://example.com) - Normalize percent-encoding (%7E \u2192 ~) - Remove fragments (#section) - Handle trailing slashes consistently (keeps them)</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to normalize</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized URL</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If URL is empty or malformed</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; URLNormalizer.normalize_url(\"HTTP://Example.COM:80/Path\")\n'http://example.com/Path'\n</code></pre> <pre><code>&gt;&gt;&gt; URLNormalizer.normalize_url(\"https://example.com/path#section\")\n'https://example.com/path'\n</code></pre> <pre><code>&gt;&gt;&gt; URLNormalizer.normalize_url(\"http://example.com:8080/path\")\n'http://example.com:8080/path'\n</code></pre>"},{"location":"api/rules/#sus.rules.URLNormalizer.filter_dangerous_schemes","title":"filter_dangerous_schemes  <code>staticmethod</code>","text":"<pre><code>filter_dangerous_schemes(url: str) -&gt; bool\n</code></pre> <p>Return True if URL scheme is safe (http/https), False otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if scheme is safe, False otherwise</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; URLNormalizer.filter_dangerous_schemes(\"http://example.com\")\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; URLNormalizer.filter_dangerous_schemes(\"mailto:user@example.com\")\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; URLNormalizer.filter_dangerous_schemes(\"javascript:alert('xss')\")\nFalse\n</code></pre>"},{"location":"api/rules/#sus.rules.URLNormalizer.handle_query_parameters","title":"handle_query_parameters  <code>staticmethod</code>","text":"<pre><code>handle_query_parameters(url: str, strategy: Literal['strip', 'preserve'] = 'strip') -&gt; str\n</code></pre> <p>Handle query parameters based on strategy.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to process</p> required <code>strategy</code> <code>Literal['strip', 'preserve']</code> <p>\"strip\" removes all query params, \"preserve\" keeps them</p> <code>'strip'</code> <p>Returns:</p> Type Description <code>str</code> <p>URL with query parameters handled according to strategy</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; URLNormalizer.handle_query_parameters(\n...     \"http://example.com/path?foo=bar&amp;baz=qux\",\n...     strategy=\"strip\"\n... )\n'http://example.com/path'\n</code></pre> <pre><code>&gt;&gt;&gt; URLNormalizer.handle_query_parameters(\n...     \"http://example.com/path?foo=bar\",\n...     strategy=\"preserve\"\n... )\n'http://example.com/path?foo=bar'\n</code></pre>"},{"location":"api/rules/#sus.rules.RulesEngine","title":"RulesEngine","text":"<pre><code>RulesEngine(config: SusConfig)\n</code></pre> <p>Evaluates crawling rules to determine which URLs to follow.</p> <p>The RulesEngine applies whitelist/blacklist patterns and depth limits to control which URLs should be crawled.</p> <p>Initialize with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SusConfig</code> <p>SusConfig containing crawling rules</p> required"},{"location":"api/rules/#sus.rules.RulesEngine-functions","title":"Functions","text":""},{"location":"api/rules/#sus.rules.RulesEngine.should_follow","title":"should_follow","text":"<pre><code>should_follow(url: str, parent_url: str | None = None) -&gt; bool\n</code></pre> <p>Determine if URL should be crawled.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to evaluate (should be normalized before calling)</p> required <code>parent_url</code> <code>str | None</code> <p>Parent URL that linked to this URL (None for start URLs)</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if URL should be crawled</p> <p>Logic: 1. Check if allowed domain 2. Check depth limit (if configured) 3. Check exclude patterns (blacklist) - return False if matched 4. Check include patterns (whitelist) - return True if matched, False if no includes</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = SusConfig(\n...     name=\"test\",\n...     site=SiteConfig(\n...         start_urls=[\"http://example.com\"],\n...         allowed_domains=[\"example.com\"]\n...     ),\n...     crawling=CrawlingRules(\n...         include_patterns=[\n...             PathPattern(pattern=\"/docs/\", type=\"prefix\")\n...         ],\n...         depth_limit=2\n...     )\n... )\n&gt;&gt;&gt; engine = RulesEngine(config)\n&gt;&gt;&gt; engine.should_follow(\"http://example.com/docs/guide\", None)\nTrue\n</code></pre>"},{"location":"api/rules/#sus.rules.LinkExtractor","title":"LinkExtractor","text":"<pre><code>LinkExtractor(selectors: list[str])\n</code></pre> <p>Extracts and normalizes links from HTML.</p> <p>Handles relative URL resolution, normalization, and filtering of extracted links.</p> <p>Initialize with CSS selectors for links.</p> <p>Parameters:</p> Name Type Description Default <code>selectors</code> <code>list[str]</code> <p>List of CSS selectors for extracting links       (e.g., [\"a[href]\", \"link[href]\"])</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; extractor = LinkExtractor([\"a[href]\"])\n&gt;&gt;&gt; extractor = LinkExtractor([\"a[href]\", \"link[href]\", \"area[href]\"])\n</code></pre> Note <p>CSS selectors are converted to XPath internally since lxml doesn't require the cssselect package for XPath.</p>"},{"location":"api/rules/#sus.rules.LinkExtractor-functions","title":"Functions","text":""},{"location":"api/rules/#sus.rules.LinkExtractor.extract_links","title":"extract_links","text":"<pre><code>extract_links(html: str, base_url: str) -&gt; set[str]\n</code></pre> <p>Extract all links from HTML.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>HTML content to extract links from</p> required <code>base_url</code> <code>str</code> <p>Base URL for resolving relative links</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of absolute, normalized URLs</p> <p>Steps: 1. Parse HTML with lxml 2. Extract links using CSS selectors 3. Convert relative to absolute using urllib.parse.urljoin() 4. Normalize each URL using URLNormalizer 5. Remove fragments 6. Filter dangerous schemes 7. Deduplicate</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; html_content = '''\n... &lt;html&gt;\n...   &lt;a href=\"/page1\"&gt;Page 1&lt;/a&gt;\n...   &lt;a href=\"http://example.com/page2\"&gt;Page 2&lt;/a&gt;\n...   &lt;a href=\"mailto:user@example.com\"&gt;Email&lt;/a&gt;\n... &lt;/html&gt;\n... '''\n&gt;&gt;&gt; extractor = LinkExtractor([\"a[href]\"])\n&gt;&gt;&gt; links = extractor.extract_links(html_content, \"http://example.com/\")\n&gt;&gt;&gt; \"http://example.com/page1\" in links\nTrue\n&gt;&gt;&gt; \"mailto:user@example.com\" in links\nFalse\n</code></pre>"},{"location":"api/scraper/","title":"Scraper","text":""},{"location":"api/scraper/#sus.scraper","title":"scraper","text":"<p>Scraper pipeline orchestration.</p> <p>Orchestrates the complete scraping workflow, coordinating crawler, converter, outputs, and assets with Rich progress display. Main entry point: run_scraper().</p>"},{"location":"api/scraper/#sus.scraper-classes","title":"Classes","text":""},{"location":"api/scraper/#sus.scraper.ScraperStats","title":"ScraperStats","text":"<p>               Bases: <code>TypedDict</code></p> <p>Type definition for scraper statistics dictionary.</p>"},{"location":"api/scraper/#sus.scraper.PagePreview","title":"PagePreview  <code>dataclass</code>","text":"<pre><code>PagePreview(url: str, output_path: str, title: str | None = None)\n</code></pre> <p>Preview information for a single page.</p>"},{"location":"api/scraper/#sus.scraper.AssetPreview","title":"AssetPreview  <code>dataclass</code>","text":"<pre><code>AssetPreview(url: str, output_path: str, asset_type: str)\n</code></pre> <p>Preview information for a single asset.</p>"},{"location":"api/scraper/#sus.scraper.PreviewReport","title":"PreviewReport  <code>dataclass</code>","text":"<pre><code>PreviewReport(pages: list[PagePreview] = list(), assets: list[AssetPreview] = list(), total_pages: int = 0, total_assets: int = 0, estimated_bytes: int = 0, config_name: str = '')\n</code></pre> <p>Complete preview report for dry-run with --preview flag.</p> <p>Contains all pages and assets that would be scraped, plus statistics. Exported as JSON for inspection before actual scraping.</p>"},{"location":"api/scraper/#sus.scraper-functions","title":"Functions","text":""},{"location":"api/scraper/#sus.scraper.run_scraper","title":"run_scraper  <code>async</code>","text":"<pre><code>run_scraper(config: SusConfig, dry_run: bool = False, max_pages: int | None = None, preview: bool = False) -&gt; dict[str, Any]\n</code></pre> <p>Run the complete scraping pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SusConfig</code> <p>Validated SusConfig instance</p> required <code>dry_run</code> <code>bool</code> <p>If True, don't write files to disk</p> <code>False</code> <code>max_pages</code> <code>int | None</code> <p>Maximum number of pages to crawl (None = unlimited)</p> <code>None</code> <code>preview</code> <code>bool</code> <p>If True, return summary without writing files</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with scraping statistics:</p> <code>dict[str, Any]</code> <ul> <li>pages_crawled: Number of pages successfully crawled</li> </ul> <code>dict[str, Any]</code> <ul> <li>pages_failed: Number of pages that failed</li> </ul> <code>dict[str, Any]</code> <ul> <li>assets_downloaded: Number of assets downloaded</li> </ul> <code>dict[str, Any]</code> <ul> <li>assets_failed: Number of assets that failed</li> </ul> <code>dict[str, Any]</code> <ul> <li>total_bytes: Total bytes downloaded</li> </ul> <code>dict[str, Any]</code> <ul> <li>execution_time: Time taken in seconds</li> </ul> <code>dict[str, Any]</code> <ul> <li>errors: Dict of error types and their occurrences</li> </ul> <code>dict[str, Any]</code> <ul> <li>files: List of file paths that were written</li> </ul> Workflow <ol> <li>Initialize components (crawler, converter, output manager, asset downloader)</li> <li>Setup Rich progress display with two progress bars</li> <li>Iterate over crawler results:</li> <li>Convert HTML to Markdown</li> <li>Rewrite links to relative paths</li> <li>Save markdown file (skip if dry_run/preview)</li> <li>Download assets for the page</li> <li>Update progress bars</li> <li>Display final summary with statistics and errors</li> <li>Return summary dict for programmatic access</li> </ol>"},{"location":"api/utils/","title":"Utils","text":""},{"location":"api/utils/#sus.utils","title":"utils","text":"<p>Utility functions.</p> <p>Utility functions for SUS, primarily logging configuration with Rich formatting for styled console output. Provides setup_logging() for RichHandler configuration.</p>"},{"location":"api/utils/#sus.utils-functions","title":"Functions","text":""},{"location":"api/utils/#sus.utils.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(verbose: bool = False) -&gt; None\n</code></pre> <p>Setup logging with RichHandler for beautiful console output.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, sets logging to DEBUG level and shows file paths.     If False, sets logging to INFO level and suppresses noisy loggers.</p> <code>False</code>"}]}